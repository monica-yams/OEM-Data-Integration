{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea7f45-445a-406a-9ad0-59cc6ab5882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "---------Delta Overwrite with Schema\n",
    "adlsCont    = \"nonprod\"\n",
    "adlsPath    = \"lake.dfs.core.windows.net\"\n",
    "\n",
    "## Name:         Oem Pipeline\n",
    "## Description:  Loads oem data\n",
    "## Author:     Monica\n",
    "\n",
    "##### Begin Notebook Execution #####\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Set Notbook Variables\n",
    "currentDt   = dt.datetime.strftime((dt.datetime.now()),\"%Y-%m-%d\")\n",
    "currentDyNm = dt.datetime.strftime((dt.datetime.now()),\"%A\").lower()\n",
    "\n",
    "adlsStagingPath  = \"abfss://\" + adlsCont + \"@\" + adlsPath + \"/lakehouse/integration/staging\"\n",
    "adlsSynergyPath  = \"abfss://\" + adlsCont + \"@\" + adlsPath + \"/lakehouse/synergy\"\n",
    "\n",
    "\n",
    "co = spark.read.load(adlsStagingPath + \"/company/*.parquet\",format = \"parquet\").where(\"RowNbr = 1\").dropDuplicates()\n",
    "co.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\",\"true\").save(adlsSynergyPath + \"/company\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--Write to Azure SQL Database\n",
    "df = spark.read.load('abfss://lakehouse@devmdpdatalake01.dfs.core.windows.net/xxx/fleet/*.parquet', format='parquet')\n",
    "#display(df.limit(10))\n",
    "\n",
    "serverNm = \"jdbc:sqlserver://xxxx\"\n",
    "databaseNm = \"xxxxx\"\n",
    "sqlSvrPath = serverNm + \";\" + \"databaseName=\" + databaseNm + \";\"\n",
    "\n",
    "tableNm = \"x.Fleet\"\n",
    "userNm = \"xxxx\"\n",
    "password = \"xxxx\"\n",
    "\n",
    "\n",
    "df.write \\\n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "  .mode(\"overwrite\").option(\"url\",sqlSvrPath) \\\n",
    "  .option(\"dbtable\",tableNm) \\\n",
    "  .option(\"user\",userNm) \\\n",
    "  .option(\"password\",password) \\\n",
    "  .save()\n",
    "\n",
    "\n",
    "--MS Spark Utility Secret\n",
    "#mssparkutils.credentials.getSecret('azure key vault name','secret name')\n",
    "# get password from key vault\n",
    "import sys\n",
    "\n",
    "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
    "password = token_library.getSecret('xxxx','xxxx')\n",
    "strA = password[0:len(password)-1]\n",
    "print(strA)\n",
    "\n",
    "-----------------------------------------------------------------------------------------------\n",
    "##### Begin Parameters #####\n",
    "\n",
    "psSystemId  = \"\"\n",
    "psLoad      = \"\"\n",
    "adlsCont    = \"\"\n",
    "adlsPath    = \"\"\n",
    "\n",
    "## Name:        Company 1\n",
    "## Description:  Loads Company 1 Api Data Into Ingest Layer (OAuth 2)\n",
    "## Author:       Monica\n",
    "\n",
    "##### Begin Notebook Execution #####\n",
    "\n",
    "# Imports\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from oauthlib.oauth2 import BackendApplicationClient\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from requests_oauthlib import OAuth2Session\n",
    "\n",
    "# Set Notbook Variables\n",
    "currentDt   = dt.datetime.strftime((dt.datetime.now()),\"%Y-%m-%d\")\n",
    "currentDyNm = dt.datetime.strftime((dt.datetime.now()),\"%A\").lower()\n",
    "\n",
    "adlsReadPath    = \"abfss://\" + adlsCont + \"@\" + adlsPath\n",
    "adlsWritePath   = \"abfss://\" + adlsCont\n",
    "adlsRemovePath  = \"abfss://\" + adlsCont + \"@\" + adlsPath\n",
    "\n",
    "yrTxt = dt.datetime.strftime((dt.datetime.now()),\"%Y\")\n",
    "moTxt = dt.datetime.strftime((dt.datetime.now()),\"%m\")\n",
    "dyTxt = dt.datetime.strftime((dt.datetime.now()),\"%d\")\n",
    "\n",
    "# Create Notbook Functions\n",
    "def dirExists(path):\n",
    "    try:\n",
    "        mssparkutils.fs.ls(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Set the OAuth2 Url\n",
    "provider_url = \"xxxxx\"\n",
    "\n",
    "# Load Config\n",
    "config = spark.read.load(adlsReadPath + \"/lakehouse/integration/configuration/api/api.txt\",format=\"csv\",header=True,sep=\"|\").where(\"SystemId = '\" + psSystemId + \"'\").dropDuplicates()\n",
    "config.createOrReplaceTempView(\"Config\")\n",
    "\n",
    "# Load Config\n",
    "cfg = spark.sql(\"Select SystemId,CoId,ClientId,Secret From Config\")\n",
    "\n",
    "# Caterpillar Fleet\n",
    "# Daily\n",
    "\n",
    "for x in cfg.collect():\n",
    "\n",
    "    # Set Cell Variables\n",
    "    client_id       = x[\"ClientId\"]\n",
    "    client_secret   = x[\"Secret\"]\n",
    "    systemId        = x[\"SystemId\"]\n",
    "    coId            = x[\"CoId\"]\n",
    "\n",
    "    #Authorization\n",
    "    client = BackendApplicationClient(client_id = client_id)\n",
    "\n",
    "    # Create an OAuth2 Session Object\n",
    "    oauth = OAuth2Session(client = client)\n",
    "\n",
    "    # Set Token\n",
    "    token = oauth.fetch_token(token_url = provider_url,auth = HTTPBasicAuth(client_id,client_secret))\n",
    "\n",
    "    b = ('Bearer', token[\"access_token\"])\n",
    "    y = (' '.join(map(str, b)))\n",
    "\n",
    "    params = {}\n",
    "    headers = {\"accept\": \"application/json\",\"Authorization\": y}\n",
    "\n",
    "    pgCnt = 0\n",
    "    hasNextPage = True\n",
    "    while hasNextPage:\n",
    "        pgCnt = pgCnt + 1 \n",
    "        try:\n",
    "            # Get Each Page Break When Does Not Exist (Throw Error)\n",
    "            response = requests.get(\"https://servicesxxxxxxxx\" + str(pgCnt), headers=headers, params=params).json()\n",
    "            f = pd.json_normalize(response[\"Equipment\"])\n",
    "            \n",
    "            # Add Alias & Default Columns\n",
    "            f[\"OemNm\"]                          = f[\"EquipmentHeader.OEMName\"]\n",
    "            f[\"LoadDt\"]                         = currentDt\n",
    "            f[\"SystemId\"]                       = systemId\n",
    "            f[\"CoId\"]                           = coId\n",
    "            f[\"SrcNm\"]                          = \"caterpillar\"\n",
    "            f[\"SrcItem\"]                        = \"fleet\"\n",
    "            f[\"EquipmentNbr\"]                   = f[\"EquipmentHeader.EquipmentID\"]\n",
    "            f[\"SerialNbr\"]                      = f[\"EquipmentHeader.SerialNumber\"]\n",
    "            f[\"Model\"]                          = f[\"EquipmentHeader.Model\"]\n",
    "            f[\"FuelConsumedDt\"]                 = f[\"FuelUsedLast24.Datetime\"]\n",
    "            f[\"FuelUnit\"]                       = f[\"FuelUsedLast24.FuelUnits\"]\n",
    "            f[\"FuelConsumed\"]                   = f[\"FuelUsedLast24.FuelConsumed\"]\n",
    "            f[\"FuelRemainingPct\"]               = f[\"FuelRemaining.Percent\"]\n",
    "            f[\"DEFRemainingPct\"]                = f[\"DEFRemaining.Percent\"]\n",
    "            f[\"FuelConsumedTot\"]                = f[\"FuelUsed.FuelConsumed\"]\n",
    "            f[\"CumulativeIdleHoursDt\"]          = f[\"CumulativeIdleHours.Datetime\"]\n",
    "            f[\"CumulativeIdleHours\"]            = f[\"CumulativeIdleHours.Hour\"]\n",
    "            f[\"CumulativeOperatingHoursDt\"]     = f[\"CumulativeOperatingHours.Datetime\"]\n",
    "            f[\"CumulativeOperatingHours\"]       = f[\"CumulativeOperatingHours.Hour\"]\n",
    "            f[\"DistanceDt\"]                     = f[\"Distance.Datetime\"]\n",
    "            f[\"DistanceUnit\"]                   = f[\"Distance.OdometerUnits\"]\n",
    "            f[\"DistanceOdometer\"]               = f[\"Distance.Odometer\"]\n",
    "            f[\"CumulativeLoadCntDt\"]            = f[\"CumulativeLoadCount.Datetime\"]\n",
    "            f[\"CumulativeLoadCnt\"]              = f[\"CumulativeLoadCount.Count\"]\n",
    "            f[\"CumulativePayloadTotDt\"]         = f[\"CumulativePayloadTotals.Datetime\"]\n",
    "            f[\"CumulativePayloadTotUnit\"]       = f[\"CumulativePayloadTotals.PayloadUnits\"]\n",
    "            f[\"CumulativePayloadTot\"]           = f[\"CumulativePayloadTotals.Payload\"]\n",
    "            \n",
    "            # Write Parquet to Data Lake (All Columns String)\n",
    "            tgtPath = adlsWritePath + \"/ingest/xxxx\" + yrTxt + \"/\" + moTxt + \"/\" + dyTxt + \"/\" + coId + \".fleet.0\" + str(pgCnt) + \".parquet\"\n",
    "            f = f.astype('string')\n",
    "            fOutput = f[[\"x\",\"y\"]].copy()\n",
    "            fOutput.to_parquet(tgtPath)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "##### End Notebook Execution #####\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
